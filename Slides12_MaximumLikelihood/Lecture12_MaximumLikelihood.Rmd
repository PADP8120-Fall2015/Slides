---
title: "Matrices and maximum likelihood estimation"
author: "Tyler Scott"
date: "2015-07-25 ![Creative Commons Attribution License](images/cc-by.png)"
output: 
  ioslides_presentation:
    css: soc504_s2015_slides.css
---

```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F, results='hide'}
# make this an external chunk that can be included in any file
library(knitr)
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
runif(1)
```

## Topics

- matrix approach to regression
- maximum likelihood approach to regression
- Bayesian inference

## Goals

After class today you will be able to 

- do matrix addition
- do matrix multiplication
- understand linear regression written in matrix notation
- understand the solution to linear regression written in matrix notation
- walk through a simple example of maximum likelihood
- talk about Bayesian statistics at a party

## Why are we doing this?

- What we are covering today will likely seem somewhat esoteric in light of the course material so far

- We are covering the matrix-based approach to regression and maximum likelihood estimation (MLE) because it will matter greatly as you progress on to other models and approaches

- Rather than learn more complex models and MLE at the same time, it makes sense to learn MLE on basic models that you readily understand

# Matrix algebra

## Matrix addition

$\begin{bmatrix} 4 & 3 & 7 \\ 3 & 8 & 2 \end{bmatrix}$ +
$\begin{bmatrix} 5 & 2 & 1\\ 2 & 1 & 5 \end{bmatrix}$ =
$\begin{bmatrix} 9 & 5 & 8\\ 5 & 9 & 7 \end{bmatrix}$ 

##

Now your turn

$\begin{bmatrix} 4 & 2 \\ 3 & 3 \\ 5 & 8 \end{bmatrix}$ +
$\begin{bmatrix} 3 & 1 \\ 7 & 2 \\ 3 & 5 \end{bmatrix}$ =

##

Now your turn

$\begin{bmatrix} 4 & 2 \\ 3 & 3 \\ 5 & 8 \end{bmatrix}$ +
$\begin{bmatrix} 3 & 1 \\ 7 & 2 \\ 3 & 5 \end{bmatrix}$ =
$\begin{bmatrix} 7 & 3 \\ 10 & 5 \\ 8 & 13 \end{bmatrix}$ 

##

Now your turn

$\begin{bmatrix} 4 & 2 & 5 \\ 3 & 3 & 7 \\ 5 & 8 & 4\end{bmatrix}$ +
$\begin{bmatrix} 3 & 1 \\ 7 & 2 \\ 3 & 5 \end{bmatrix}$ = 

##

Now your turn

$\begin{bmatrix} 4 & 2 & 5 \\ 3 & 3 & 7 \\ 5 & 8 & 4\end{bmatrix}$ +
$\begin{bmatrix} 3 & 1 \\ 7 & 2 \\ 3 & 5 \end{bmatrix}$ = $\mathbf{NA}$

Matrix addition only works if the matricies are the same dimension (this foreshadows a related property for matrix multiplication)

## matrix multiplication

Example from [Wikipedia](http://en.wikipedia.org/wiki/Matrix_multiplication)

$\begin{bmatrix}
1 & 0 & 2 \\ 
-1 & 3 & 1\\
\end{bmatrix}$
$\begin{bmatrix} 
3 & 1 \\ 
2 & 1 \\ 
1 & 0 \\
\end{bmatrix}$
=
$\begin{bmatrix}1 \times 3 + 0 \times 2 + 2 \times 1 & 1 \times 1 + 0 \times 2 + 2 \times 1\\
-1 \times 3 + 3 \times 2 + 1 \times 1 & -1 \times 1 + 3 \times 1 + 1 \times 0 \end{bmatrix}$
=
$\begin{bmatrix} 5 & 1\\
4 & 2 \end{bmatrix}$

Note that $2 \times 3 \cdot 3 \times 2 = 2 \times 2$

##

More generally
$(\mathbf{AB})_{i,j} = \sum_{r=1}^n A_{i,r}B_{r,j}$ 

$\mathbf{A} = \begin{bmatrix}
1 & 0 & 2 \\ 
-1 & 3 & 1\\
\end{bmatrix}$

$\mathbf{B} =\begin{bmatrix} 
3 & 1 \\ 
2 & 1 \\ 
1 & 0 \\
\end{bmatrix}$

$\mathbf{AB} = 
\begin{bmatrix} 5 & 1\\
4 & 2 \end{bmatrix}$

##

If $A$ is $(r_1 \times c_1)$ and $B$ is $(r_2 \times c_2)$ then they can be multiplied if and only if $r_2 = c_1$.  If they can be multiplied the result is $r_1 \times c_2$

$\begin{bmatrix} 
3 & 1 \\ 
2 & 1 \\ 
1 & 0 \\
\end{bmatrix}$
$\begin{bmatrix}
1 & 0 & 2 \\ 
-1 & 3 & 1\\
\end{bmatrix}$

- What size is the first matrix?
- What size is the second matrix?
- Can they multiplied together?  If so, what will be the size of the result.
- Calculate the result.

##

$\begin{bmatrix} 
3 & 1 \\ 
2 & 1 \\ 
1 & 0 \\
\end{bmatrix}$
$\begin{bmatrix}
1 & 0 & 2 \\ 
-1 & 3 & 1\\
\end{bmatrix}$
=
$\begin{bmatrix}
3 \times 1 + 1 \times -1 & 3 \times 0 + 1 \times 3 & 3 \times 2 + 1 \times 1 \\
2 \times 1 + 1 \times -1 & 2 \times 0 + 1 \times 3 & 2 \times 2 + 1 \times 1 \\
1 \times 1 + 0 \times -1 & 1 \times 0 + 0 \times 3 & 1 \times 2 + 0 \times 1 \\
\end{bmatrix}$
=
$\begin{bmatrix}
2 & 3 & 7 \\
1 & 3 & 5 \\
1 & 0 & 2 \\
\end{bmatrix}$


- What size is the first matrix?: 3 x 2
- What size is the second matrix?: 2 x 3
- Can they multiplied together?  If so, what will be the size of the result: 3 x 3


# matrices and regression

##  Regression equation

$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i$

For 5 people this would be

$y_1 = \beta_0 + \beta_1 x_{11} + \beta_2 x_{12} + \beta_3 x_{13} + \epsilon_1$

$y_2 = \beta_0 + \beta_1 x_{21} + \beta_2 x_{22} + \beta_3 x_{23} + \epsilon_2$

$y_3 = \beta_0 + \beta_1 x_{31} + \beta_2 x_{32} + \beta_3 x_{33} + \epsilon_3$

$y_4 = \beta_0 + \beta_1 x_{41} + \beta_2 x_{42} + \beta_3 x_{43} + \epsilon_4$

$y_5 = \beta_0 + \beta_1 x_{51} + \beta_2 x_{52} + \beta_3 x_{53} + \epsilon_5$

##

$\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \end{bmatrix}$ = 
$\begin{bmatrix} 
 1 & x_{11} & x_{12} & x_{13} \\
 1 & x_{21} & x_{22} & x_{23} \\
 1 & x_{31} & x_{32} & x_{33} \\
 1 & x_{41} & x_{42} & x_{43} \\
 1 & x_{51} & x_{52} & x_{53} \\
\end{bmatrix}$
$\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \end{bmatrix}$ +
$\begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \\ \epsilon_5 \end{bmatrix}$

$y_1 = \beta_0 + \beta_1 x_{11} + \beta_2 x_{12} + \beta_3 x_{13} + \epsilon_1$

$y_2 = \beta_0 + \beta_1 x_{21} + \beta_2 x_{22} + \beta_3 x_{23} + \epsilon_2$

$y_3 = \beta_0 + \beta_1 x_{31} + \beta_2 x_{32} + \beta_3 x_{33} + \epsilon_3$

$y_4 = \beta_0 + \beta_1 x_{41} + \beta_2 x_{42} + \beta_3 x_{43} + \epsilon_4$

$y_5 = \beta_0 + \beta_1 x_{51} + \beta_2 x_{52} + \beta_3 x_{53} + \epsilon_5$


## Remember our matrix algebra rules:

- Can multiply when $c_1$ = $r_2$

    - in this case $c_1 = 4$, and $r_2 = 4$

- When multiplying, product dimension is: $r_1 \times c_2$

## Condensing notation

- We can take our equation:

$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i$

- and rewrite as:

$\mathbf{Y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$

- Each term can then be represented via a matrix...

## Dependent variable

$\mathbf{Y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$

$\mathbf{y} = \begin{bmatrix} Y_1 \\ Y_2 \\ Y_3 \\ Y_4 \\ Y_5 \end{bmatrix}$

## Independent variable(s)

$\mathbf{Y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$

$\mathbf{X} = \begin{bmatrix} 
 1 & x_{11} & x_{12} & x_{13} \\
 1 & x_{21} & x_{22} & x_{23} \\
 1 & x_{31} & x_{32} & x_{33} \\
 1 & x_{41} & x_{42} & x_{43} \\
 1 & x_{51} & x_{52} & x_{53} \\
\end{bmatrix}$


## Coefficient(s)

$\mathbf{Y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$

$\mathbf{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \end{bmatrix}$

## Residuals (error terms)

$\mathbf{Y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$

$\mathbf{\epsilon} = \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \\ \epsilon_5 \end{bmatrix}$


## Looking under the hood 

- This...

$\mathbf{Y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$

- Is really a stand-in for...

$\mathbf{y} = \begin{bmatrix} Y_1 \\ Y_2 \\ Y_3 \\ Y_4 \\ Y_5 \end{bmatrix} \sim \begin{bmatrix} 
 1 & x_{11} & x_{12} & x_{13} \\
 1 & x_{21} & x_{22} & x_{23} \\
 1 & x_{31} & x_{32} & x_{33} \\
 1 & x_{41} & x_{42} & x_{43} \\
 1 & x_{51} & x_{52} & x_{53} \\
\end{bmatrix} \times \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \end{bmatrix} + \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \\ \epsilon_5 \end{bmatrix}$

## Known and unknown

- We know $\mathbf{Y}$ and $\mathbf{X}$ 

- We want to solve for $\mathbf{\beta}$

- When we do, we will end up with $\mathbf{\epsilon}$ as well


## solving regression in matrix notation

$\mathbf{y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$

Want to find $\mathbf{\hat{\beta}}$ that minimizes the residual sum of squares:

$$RSS = \sum_{i=1}^n (y_i-\mathbf{X} \mathbf{\beta})^2$$

It turns out that when estimating \mathbf{\beta} 

$$\mathbf{\hat{\beta}} = \mathbf{(X'X)^{-1}X'y}$$

To understand this we need to learn two new matrix operations

- $X'$ (transpose)
- $(X'X)^{-1}$ (inverse)

## Transpose

The transpose is an operation that simply changes columns to rows. We use either a $\top$ or $'$ to denote transpose. Here is the technical definition. If X is $r_1 \times c_1$:
```{r echo=FALSE}
X <- matrix(1:12,4,3)
print(X)
```

the transpose ($X'$) will be $c_1 \times r_1$:

```{r echo=FALSE}
t(X)
```

## More formally

$$ \mathbf{X'} = \begin{pmatrix} x_{1,1}&\dots & x_{1,p} \ x_{2,1}&\dots & x_{2,p} \ & \vdots & \ x_{N,1}&\dots & x_{N,p} \ \end{pmatrix} \implies \mathbf{X}^\top = $$

$$\begin{pmatrix} x_{1,1}&\dots & x_{p,1} \ x_{1,2}&\dots & x_{p,2} \ & \vdots & \ x_{1,N}&\dots & x_{p,N} \ \end{pmatrix} $$

## Inverse

The inverse of matrix of $X$, denoted with $X^{-1}$ has the property that when multiplied give you the identity $X^{-1}X=I$. Note that not all matrices have inverses. For example a $2\times 2$ matrix with 1s in all it's entries does not have an inverse.

As we will see when we get to the applications to linear models, being able to compute the inverse of a matrix is quite useful.


# Likelihood

## Likelihood

- A common and fruitful approach to statistics is to assume that the data arises from a family of distributions indexed by a parameter that represents a useful summary of the distribution
- The **likelihood** of a collection of data is the joint density evaluated as a function of the parameters with the data fixed
- Likelihood analysis of data uses the likelihood to perform inference regarding the unknown parameter


## Likelihood

Given a statistical probability mass function or density, say $f(x, \theta)$, where $\theta$ is an unknown parameter, the **likelihood** is $f$ viewed as a function of $\theta$ for a fixed, observed value of $x$. 


## Interpretations of likelihoods

The likelihood has the following properties:

1. Ratios of likelihood values measure the relative evidence of one value of the unknown parameter to another.
2. Given a statistical model and observed data, all of the relevant information contained in the data regarding the unknown parameter is contained in the likelihood.
3. If $\{X_i\}$ are independent random variables, then their likelihoods multiply.  That is, the likelihood of the parameters given all of the $X_i$ is simply the product of the individual likelihoods.


## Example

- Suppose that we flip a coin with success probability $\theta$
- Recall that the mass function for $x$
  $$
  f(x,\theta) = \theta^x(1 - \theta)^{1 - x}  ~~~\mbox{for}~~~ \theta \in [0,1].
  $$
  where $x$ is either $0$ (Tails) or $1$ (Heads) 
- Suppose that the result is a head
- The likelihood is
  $$
  {\cal L}(\theta, 1) = \theta^1 (1 - \theta)^{1 - 1} = \theta  ~~~\mbox{for} ~~~ \theta \in [0,1].
  $$
- Therefore, ${\cal L}(.5, 1) / {\cal L}(.25, 1) = 2$, 
- There is twice as much evidence supporting the hypothesis that $\theta = .5$ to the hypothesis that $\theta = .25$


## Example continued

- Suppose now that we flip our coin from the previous example 4 times and get the sequence 1, 0, 1, 1
- The likelihood is:
$$
  \begin{eqnarray*}
  {\cal L}(\theta, 1,0,1,1) & = & \theta^1 (1 - \theta)^{1 - 1}
  \theta^0 (1 - \theta)^{1 - 0}  \\
& \times & \theta^1 (1 - \theta)^{1 - 1} 
   \theta^1 (1 - \theta)^{1 - 1}\\
& = &  \theta^3(1 - \theta)^1
  \end{eqnarray*}
$$
- This likelihood only depends on the total number of heads and the total number of tails; we might write ${\cal L}(\theta, 1, 3)$ for shorthand
- Now consider ${\cal L}(.5, 1, 3) / {\cal L}(.25, 1, 3) = 5.33$
- There is over five times as much evidence supporting the hypothesis that $\theta = .5$ over that $\theta = .25$


## Plotting likelihoods

- Generally, we want to consider all the values of $\theta$ between 0 and 1
- A **likelihood plot** displays $\theta$ by ${\cal L}(\theta,x)$
- Because the likelihood measures *relative evidence*, dividing the curve by its maximum value (or any other value for that matter) does not change its interpretation

##

```{r, fig.height=4.5, fig.width=4.5}
pvals <- seq(0, 1, length = 1000)
plot(pvals, dbinom(3, 4, pvals) / dbinom(3, 4, 3/4), type = "l", frame = FALSE, lwd = 3, xlab = "p", ylab = "likelihood / max likelihood")
```


## Maximum likelihood

- The value of $\theta$ where the curve reaches its maximum has a special meaning
- It is the value of $\theta$ that is most well supported by the data
- This point is called the **maximum likelihood estimate** (or MLE) of $\theta$
  $$
  MLE = \mathrm{argmax}_\theta {\cal L}(\theta, x).
  $$
- Another interpretation of the MLE is that it is the value of $\theta$ that would make the data that we observed most probable


## Some results
* $X_1, \ldots, X_n \stackrel{iid}{\sim} N(\mu, \sigma^2)$ the MLE of $\mu$ is $\bar X$ and the ML of $\sigma^2$ is the biased sample variance estimate.
* If $X_1,\ldots, X_n \stackrel{iid}{\sim} Bernoulli(p)$ then the MLE of $p$ is $\bar X$ (the sample proportion of 1s).
* If $X_i \stackrel{iid}{\sim} Binomial(n_i, p)$ then the MLE of $p$ is $\frac{\sum_{i=1}^n X_i}{\sum_{i=1}^n n_i}$ (the sample proportion of 1s).
* If $X \stackrel{iid}{\sim} Poisson(\lambda t)$ then the MLE of $\lambda$ is $X/t$.
* If $X_i \stackrel{iid}{\sim} Poisson(\lambda t_i)$ then the MLE of $\lambda$ is
  $\frac{\sum_{i=1}^n X_i}{\sum_{i=1}^n t_i}$


## Example {.smaller}
* You saw 5 failure events per 94 days of monitoring a nuclear pump. 
* Assuming Poisson, plot the likelihood

```{r, fig.height=4, fig.width=4, echo= FALSE}
lambda <- seq(0, .2, length = 1000)
likelihood <- dpois(5, 94 * lambda) / dpois(5, 5)
plot(lambda, likelihood, frame = FALSE, lwd = 3, type = "l", xlab = expression(lambda))
lines(rep(5/94, 2), 0 : 1, col = "red", lwd = 3)
lines(range(lambda[likelihood > 1/16]), rep(1/16, 2), lwd = 2)
lines(range(lambda[likelihood > 1/8]), rep(1/8, 2), lwd = 2)
```

# Bayesian detour

## Bayesian analysis
- Bayesian statistics posits a *prior* on the parameter
  of interest
- All inferences are then performed on the distribution of 
  the parameter given the data, called the posterior
- In general,
  $$
  \mbox{Posterior} \propto \mbox{Likelihood} \times \mbox{Prior}
  $$
- The likelihood is the factor by which our prior beliefs are updated to produce
  conclusions in the light of the data

## Prior specification
- The beta distribution is the default prior
  for parameters between $0$ and $1$.
- The beta density depends on two parameters $\alpha$ and $\beta$
$$
\frac{\Gamma(\alpha +  \beta)}{\Gamma(\alpha)\Gamma(\beta)}
 p ^ {\alpha - 1} (1 - p) ^ {\beta - 1} ~~~~\mbox{for} ~~ 0 \leq p \leq 1
$$
- The mean of the beta density is $\alpha / (\alpha + \beta)$
- The variance of the beta density is 
$$\frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$$
- The uniform density is the special case where $\alpha = \beta = 1$


## Posterior 
- Suppose that we chose values of $\alpha$ and $\beta$ so that
  the beta prior is indicative of our degree of belief regarding $p$
  in the absence of data
- Then using the rule that
  $$
  \mbox{Posterior} \propto \mbox{Likelihood} \times \mbox{Prior}
  $$
  and throwing out anything that doesn't depend on $p$, we have that
$$
\begin{align}
\mbox{Posterior} &\propto  p^x(1 - p)^{n-x} \times p^{\alpha -1} (1 - p)^{\beta - 1} \\
                 &  =      p^{x + \alpha - 1} (1 - p)^{n - x + \beta - 1}
\end{align}
$$
- This density is just another beta density with parameters
  $\tilde \alpha = x + \alpha$ and $\tilde \beta = n - x + \beta$

## Posterior mean

$$
\begin{align}
E[p ~|~ X] & =   \frac{\tilde \alpha}{\tilde \alpha + \tilde \beta}\\ \\
& =  \frac{x + \alpha}{x + \alpha + n - x + \beta}\\ \\
& =  \frac{x + \alpha}{n + \alpha + \beta} \\ \\
& =  \frac{x}{n} \times \frac{n}{n + \alpha + \beta} + \frac{\alpha}{\alpha + \beta} \times \frac{\alpha + \beta}{n + \alpha + \beta} \\ \\
& =  \mbox{MLE} \times \pi + \mbox{Prior Mean} \times (1 - \pi)
\end{align}
$$

## Thoughts

- The posterior mean is a mixture of the MLE ($\hat p$) and the
  prior mean
- $\pi$ goes to $1$ as $n$ gets large; for large $n$ the data swamps the prior
- For small $n$, the prior mean dominates 
- Generalizes how science should ideally work; as data becomes
  increasingly available, prior beliefs should matter less and less
- With a prior that is degenerate at a value, no amount of data
  can overcome the prior

## Example I

- Suppose that in a random sample of an at-risk population
$13$ of $20$ subjects had hypertension. Estimate the prevalence
of hypertension in this population.
- $x = 13$ and $n=20$
- Consider a uniform prior, $\alpha = \beta = 1$
- The posterior is proportional to (see formula above)
$$
p^{x + \alpha - 1} (1 - p)^{n - x + \beta - 1} = p^x (1 - p)^{n-x}
$$
That is, for the uniform prior, the posterior is the likelihood...

## alpha = 1, beta = 1
```{r echo=FALSE}
pvals <- seq(0.01, 0.99, length = 1000)
x <- 13; n <- 20
myPlot <- function(alpha, beta){
    plot(0 : 1, 0 : 1, type = "n", xlab = "p", ylab = "", frame = FALSE)
    lines(pvals, dbeta(pvals, alpha, beta) / max(dbeta(pvals, alpha, beta)), 
            lwd = 3, col = "darkred")
    lines(pvals, dbinom(x,n,pvals) / dbinom(x,n,x/n), lwd = 3, col = "darkblue")
    lines(pvals, dbeta(pvals, alpha+x, beta+(n-x)) / max(dbeta(pvals, alpha+x, beta+(n-x))),
        lwd = 3, col = "darkgreen")
    title("red=prior,green=posterior,blue=likelihood")
}
myPlot(1,1)
```

## Example II

Consider the instance where $\alpha = \beta = 2$ (this prior
is humped around the point $.5$) the posterior is
$$
p^{x + \alpha - 1} (1 - p)^{n - x + \beta - 1} = p^{x + 1} (1 - p)^{n-x + 1}
$$

## alpha = 2, beta = 2
```{r echo=FALSE}
pvals <- seq(0.01, 0.99, length = 1000)
x <- 13; n <- 20
myPlot <- function(alpha, beta){
    plot(0 : 1, 0 : 1, type = "n", xlab = "p", ylab = "", frame = FALSE)
    lines(pvals, dbeta(pvals, alpha, beta) / max(dbeta(pvals, alpha, beta)), 
            lwd = 3, col = "darkred")
    lines(pvals, dbinom(x,n,pvals) / dbinom(x,n,x/n), lwd = 3, col = "darkblue")
    lines(pvals, dbeta(pvals, alpha+x, beta+(n-x)) / max(dbeta(pvals, alpha+x, beta+(n-x))),
        lwd = 3, col = "darkgreen")
    title("red=prior,green=posterior,blue=likelihood")
}
myPlot(2,2)
```

## Credible intervals
- A Bayesian credible interval is the  Bayesian analog of a confidence
  interval
- A $95\%$ credible interval, $[a, b]$ would satisfy
  $$
  P(p \in [a, b] ~|~ x) = .95
  $$

# wrap-up

## Questions?

## 

Goal check

##


